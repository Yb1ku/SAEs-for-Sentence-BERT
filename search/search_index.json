{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quick start","text":"<p>\u26a0\ufe0fWARNING  This page is still under construction. </p> <p>This codebase exists to provide a simple environment for :  - Training Sparse Autoencoders (SAEs) on Sentence-BERT embeddings.  - Analyzing the features obtained from the SAEs. </p>"},{"location":"#quick-start","title":"QUICK START","text":"<p>Install the requirements: </p> <pre><code>pip install -r requirements.txt \n</code></pre>"},{"location":"#load-a-sparse-autoencoder","title":"Load a Sparse Autoencoder","text":"<p>The following snippet shows how to load a Sparse Autoencoder from a wandb project. </p> <pre><code>from config import get_default_cfg \nfrom explainer import Explainer\n\ncfg = get_default_cfg()\ncfg[\"artifact_path\"] = 'your_artifact_path' # Path to the artifact in wandb \n\nexplainer = Explainer(cfg)\nsae = explainer.load_sae()\n</code></pre>"},{"location":"#sparse-autoencoder-analysis","title":"Sparse Autoencoder Analysis","text":"<p>The following snippet shows how to analyze the features obtained from the Sparse Autoencoder. You can  compute the fire rate of each feature, get the top activating texts for each feature and extract the  keywords which describe them. </p> <pre><code>fire_rate = explainer.compute_fire_rate(column=\"text\", save_path=\"fire_rate.npy\")\ntop_activations = explainer.get_top_activating_texts(\n    num_examples = 100,\n    top_k = 2,\n    save_to_json = True\n)\nkeywords = explainer.extract_keywords_per_feature(\n    top_activations, top_n_keywords = 3\n)\n</code></pre> <p><code>top_activations</code> is a dictionary, each key corresponding to a feature. Inside each key, there is a list  of tuples of the form <code>[(activation), text]</code>. <code>activation</code> is the activation produced by the text to the  feature, and <code>text</code> is the text itself. For example, a possible output of <code>top_activations</code> could be: </p> <pre><code>[[2.9942984580993652,\n  '  Trust region policy optimization (TRPO) is a popular and empirically\\nsuccessful policy search \n  algorithm in Reinforcement Learning (RL) in which a\\nsurrogate problem, that restricts consecutive \n  policies to be \\'close\\' to one\\nanother, is iteratively solved. Nevertheless, TRPO has been considered \n  a\\nheuristic algorithm inspired by Conservative Policy Iteration (CPI). We show\\nthat the adaptive \n  scaling mechanism used in TRPO is in fact the natural \"RL\\nversion\" of traditional trust-region methods \n  from convex analysis. We first\\nanalyze TRPO in the planning setting, in which we have access to the \n  model and\\nthe entire state space. Then, we consider sample-based TRPO and \n  establish\\n$\\\\tilde O(1/\\\\sqrt{N})$ convergence rate to the global optimum. Importantly, \n  the\\nadaptive scaling mechanism allows us to analyze TRPO in regularized MDPs for\\nwhich we prove fast \n  rates of $\\\\tilde O(1/N)$, much like results in convex\\noptimization. This is the first result in RL \n  of better rates when regularizing\\nthe instantaneous cost or reward.\\n'],\n [2.9936885833740234,\n  '  We present a reinforcement learning (RL) framework to synthesize a control\\npolicy from a given linear\n   temporal logic (LTL) specification in an unknown\\nstochastic environment that can be modeled as a Markov\n    Decision Process (MDP).\\nSpecifically, we learn a policy that maximizes the probability of \n    satisfying\\nthe LTL formula without learning the transition probabilities. We introduce a\\nnovel \n    rewarding and path-dependent discounting mechanism based on the LTL\\nformula such that (i) an optimal \n    policy maximizing the total discounted reward\\neffectively maximizes the probabilities of satisfying \n    LTL objectives, and (ii)\\na model-free RL algorithm using these rewards and discount factors \n    is\\nguaranteed to converge to such policy. Finally, we illustrate the applicability\\nof our RL-based \n    synthesis approach on two motion planning case studies.\\n']]\n</code></pre> <p><code>keywords</code> has a similar structure. It is also a dictionary, each key corresponding to a feature.  Inside each key, there is a list of tuples of the form <code>[(keyword), score]</code>. <code>keyword</code> is the keyword itself,  and <code>score</code> is the score of the keyword. For example, a possible output of <code>keywords</code> could be: </p> <pre><code>{'feature_id': 2220,\n 'keywords': [['reinforcement', 1.620540681324779],\n  ['reinforcement learning', 0.9814733623507801],\n  ['reward optimization', 0.9179529960192596]} \n</code></pre>"}]}