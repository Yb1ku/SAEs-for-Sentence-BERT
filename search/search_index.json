{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quick start","text":"<p>\u26a0\ufe0fWARNING  This page is still under construction. </p> <p>This codebase exists to provide a simple environment for :  - Training Sparse Autoencoders (SAEs) on Sentence-BERT embeddings.  - Analyzing the features obtained from the SAEs. </p>"},{"location":"#quick-start","title":"QUICK START","text":"<p>Install the requirements: </p> <pre><code>pip install -r requirements.txt \n</code></pre>"},{"location":"#load-a-sparse-autoencoder","title":"Load a Sparse Autoencoder","text":"<p>The following snippet shows how to load a Sparse Autoencoder from a wandb project. </p> <pre><code>from config import get_default_cfg \nfrom explainer import Explainer\n\ncfg = get_default_cfg()\ncfg[\"artifact_path\"] = 'your_artifact_path' # Path to the artifact in wandb \n\nexplainer = Explainer(cfg)\nsae = explainer.load_sae()\n</code></pre>"},{"location":"#sparse-autoencoder-analysis","title":"Sparse Autoencoder Analysis","text":"<p>The following snippet shows how to analyze the features obtained from the Sparse Autoencoder. You can  compute the fire rate of each feature, get the top activating texts for each feature and extract the  keywords which describe them. </p> <pre><code>fire_rate = explainer.compute_fire_rate(column=\"text\", save_path=\"fire_rate.npy\")\ntop_activations = explainer.get_top_activating_texts(\n    num_examples = 100,\n    top_k = 2,\n    save_to_json = True\n)\nkeywords = explainer.extract_keywords_per_feature(\n    top_activations, top_n_keywords = 3\n)\n</code></pre> <p><code>top_activations</code> is a dictionary, each key corresponding to a feature. Inside each key, there is a list  of tuples of the form <code>[(activation), text]</code>. <code>activation</code> is the activation produced by the text to the  feature, and <code>text</code> is the text itself. For example, a possible output of <code>top_activations</code> could be: </p> <pre><code>[[2.9942984580993652,\n  '  Trust region policy optimization (TRPO) is a popular and empirically\\nsuccessful policy search \n  algorithm in Reinforcement Learning (RL) in which a\\nsurrogate problem, that restricts consecutive \n  policies to be \\'close\\' to one\\nanother, is iteratively solved. Nevertheless, TRPO has been considered \n  a\\nheuristic algorithm inspired by Conservative Policy Iteration (CPI). We show\\nthat the adaptive \n  scaling mechanism used in TRPO is in fact the natural \"RL\\nversion\" of traditional trust-region methods \n  from convex analysis. We first\\nanalyze TRPO in the planning setting, in which we have access to the \n  model and\\nthe entire state space. Then, we consider sample-based TRPO and \n  establish\\n$\\\\tilde O(1/\\\\sqrt{N})$ convergence rate to the global optimum. Importantly, \n  the\\nadaptive scaling mechanism allows us to analyze TRPO in regularized MDPs for\\nwhich we prove fast \n  rates of $\\\\tilde O(1/N)$, much like results in convex\\noptimization. This is the first result in RL \n  of better rates when regularizing\\nthe instantaneous cost or reward.\\n'],\n [2.9936885833740234,\n  '  We present a reinforcement learning (RL) framework to synthesize a control\\npolicy from a given linear\n   temporal logic (LTL) specification in an unknown\\nstochastic environment that can be modeled as a Markov\n    Decision Process (MDP).\\nSpecifically, we learn a policy that maximizes the probability of \n    satisfying\\nthe LTL formula without learning the transition probabilities. We introduce a\\nnovel \n    rewarding and path-dependent discounting mechanism based on the LTL\\nformula such that (i) an optimal \n    policy maximizing the total discounted reward\\neffectively maximizes the probabilities of satisfying \n    LTL objectives, and (ii)\\na model-free RL algorithm using these rewards and discount factors \n    is\\nguaranteed to converge to such policy. Finally, we illustrate the applicability\\nof our RL-based \n    synthesis approach on two motion planning case studies.\\n']]\n</code></pre> <p><code>keywords</code> has a similar structure. It is also a dictionary, each key corresponding to a feature.  Inside each key, there is a list of tuples of the form <code>[(keyword), score]</code>. <code>keyword</code> is the keyword itself,  and <code>score</code> is the score of the keyword. For example, a possible output of <code>keywords</code> could be: </p> <pre><code>{'feature_id': 2220,\n 'keywords': [['reinforcement', 1.620540681324779],\n  ['reinforcement learning', 0.9814733623507801],\n  ['reward optimization', 0.9179529960192596]} \n</code></pre>"},{"location":"activationstore/","title":"Activation Store","text":"<p>Note: The code implemented in this section is based on the implementation  available here.  For this project, I have only modified it to adapt it to the <code>SentenceBERT</code> model. </p> <p>The <code>ActivationsStoreSBERT</code> class is a utility for efficiently extracting  and buffering dense <code>Sentence-BERT</code> embeddings from a streaming dataset.  It is used as an intermediate component between a sentence encoder and a  sparse autoencoder.</p>"},{"location":"activationstore/#purpose","title":"\ud83e\udde0 Purpose","text":"<p>Its main goal is to: - Stream examples from a large dataset. - Encode them using a Sentence-BERT model. - Buffer multiple batches of embeddings into memory. - Serve these embeddings as mini-batches for training or analysis.</p> <p>This is particularly useful when working with large-scale streaming datasets  (e.g. Hugging Face <code>load_dataset(..., streaming=True)</code>) that do not fit in memory.</p>"},{"location":"activationstore/#configuration-parameters","title":"\u2699\ufe0f Configuration Parameters","text":"Argument Description <code>model</code> A Sentence-BERT model with <code>.encode()</code> method (e.g. from <code>sentence-transformers</code>). <code>cfg</code> A dictionary containing the following keys: <code>dataset_path</code> Path to the Hugging Face dataset to load. <code>model_batch_size</code> Number of examples processed at once by the SBERT model. <code>device</code> <code>\"cuda\"</code> or <code>\"cpu\"</code> device for embedding extraction. <code>num_batches_in_buffer</code> Number of batches to pre-load and concatenate into an activation buffer. <code>num_examples</code> Total number of examples to extract. Acts as a cap to avoid exhausting memory."},{"location":"activationstore/#main-methods","title":"\ud83e\uddea Main Methods","text":""},{"location":"activationstore/#get_batch_tokens","title":"<code>get_batch_tokens()</code>","text":"<p>Streams a batch of raw text examples from the dataset. Raises an error if the dataset is exhausted or the example cap is reached.</p>"},{"location":"activationstore/#get_activationstexts","title":"<code>get_activations(texts)</code>","text":"<p>Encodes a list of texts into dense embeddings using the SBERT model.</p>"},{"location":"activationstore/#_fill_buffer","title":"<code>_fill_buffer()</code>","text":"<p>Fills an internal activation buffer with multiple batches of embeddings. This is used to simulate an in-memory dataset for later sampling.</p>"},{"location":"activationstore/#_get_dataloader","title":"<code>_get_dataloader()</code>","text":"<p>Creates a PyTorch <code>DataLoader</code> over the activation buffer for minibatch training or analysis.</p>"},{"location":"activationstore/#next_batch","title":"<code>next_batch()</code>","text":"<p>Returns the next minibatch of SBERT embeddings. If the internal buffer is exhausted, it is automatically refreshed by re-encoding new examples.</p>"},{"location":"activationstore/#lifecycle-and-usage","title":"\ud83d\udd01 Lifecycle and Usage","text":"<p>Typical usage pattern: ```python store = ActivationsStoreSBERT(model=sbert, cfg=config) batch = store.next_batch()  # returns a tensor of SBERT embeddings</p>"},{"location":"saes/","title":"SAE models","text":"<p>Note  The SAE models used in this project were not developed from scratch by me.  They are extracted from an existing implementation available at:  https://github.com/bartbussmann/BatchTopK.  The focus of this project is not the implementation and training of the SAEs, but the developement  of a method for interpreting the features obtained from them. </p> <p>There are a total of 4 different SAE implementations:  - <code>VanillaSAE</code>: Original SAE implementation.  - <code>JumpReLU</code>: SAE with <code>JumpReLU</code> activation function.  - <code>BatchTopK</code>: SAE with <code>BatchTopK</code> activation function.  - <code>BatchTopKJumpReLU</code>: SAE with <code>BatchTopK</code> for training and <code>JumpReLU</code> for inference. </p>"},{"location":"training/","title":"\ud83d\udd27 SAE Training Script","text":"<p>This script defines the training pipeline for various Sparse Autoencoder (SAE) architectures over Sentence-BERT embeddings. It allows configuration of the SAE variant, dataset, training parameters, and model behavior.</p>"},{"location":"training/#overview","title":"\ud83d\udce6 Overview","text":"<p>The script performs the following steps:</p> <ol> <li>Load a pre-trained Sentence-BERT model.</li> <li>Stream text data from a dataset.</li> <li>Encode the texts into dense embeddings.</li> <li>Train a selected SAE variant on these embeddings.</li> </ol>"},{"location":"training/#configuration-get_experiment_cfg","title":"\u2699\ufe0f Configuration: <code>get_experiment_cfg()</code>","text":"<p>The function <code>get_experiment_cfg()</code> sets up the experiment configuration.  It returns a dictionary (<code>cfg</code>) with training hyperparameters and system settings:</p> Key Description <code>sae_type</code> SAE variant to use (<code>vanilla</code>, <code>topk</code>, <code>batchtopk</code>, <code>jumprelu</code>) <code>model_name</code> Pre-trained SBERT model name (Hugging Face) <code>dataset_path</code> Hugging Face dataset path (streamed) <code>dict_size</code> Number of latent features (SAE output dimensionality) <code>top_k</code> Number of active features allowed in the SAE output <code>lr</code> Learning rate <code>aux_penalty</code> Auxiliary penalty (e.g., JumpReLU regularization) <code>input_unit_norm</code> Whether to normalize input embeddings <code>device</code> <code>\"cuda\"</code> or <code>\"cpu\"</code> <code>wandb_project</code> Project name for Weights &amp; Biases logging <p>The config is finalized by <code>post_init_cfg(cfg)</code>, which fills in  derived or default values.</p>"},{"location":"training/#sae-selection","title":"\ud83e\udde0 SAE Selection","text":"<p>The script dynamically selects the appropriate SAE class:</p> <pre><code>sae_classes = {\n    \"vanilla\": VanillaSAE,\n    \"topk\": TopKSAE,\n    \"batchtopk\": BatchTopKSAE,\n    \"jumprelu\": JumpReLUSAE,\n}\nsae = sae_classes[cfg[\"sae_type\"]](cfg)\n</code></pre> <p>Each SAE receives the same configuration dictionary.</p>"},{"location":"training/#data-loading-activationsstoresbert","title":"\ud83d\udcca Data Loading: <code>ActivationsStoreSBERT</code>","text":"<p>The class <code>ActivationsStoreSBERT</code> is responsible for efficiently streaming and  buffering text data as Sentence-BERT embeddings.</p> <p>It performs the following tasks:</p> <ul> <li>Streams raw text from a Hugging Face dataset using <code>streaming=True</code>.</li> <li>Encodes text batches using a pre-trained Sentence-BERT model.</li> <li>Buffers multiple batches into memory to simulate a fixed-size dataset.</li> <li>Provides minibatches of embeddings for training or analysis.</li> </ul> <pre><code>activation_store = ActivationsStoreSBERT(model, cfg)\n</code></pre> <p>This design enables large-scale training without loading the entire dataset into memory. It also handles iteration limits and buffer regeneration automatically.</p>"}]}