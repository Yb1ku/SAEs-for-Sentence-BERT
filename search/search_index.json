{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quick start","text":"<p>\u26a0\ufe0fWARNING  This page is still under construction. </p> <p>This codebase exists to provide a simple environment for:</p> <ul> <li>Training Sparse Autoencoders (SAEs) on Sentence-BERT embeddings. </li> <li>Analyzing the features obtained from the SAEs. </li> <li>Present a method for interpreting mono-semantic features in SAEs using keyword extraction via KeyBERT. </li> </ul>"},{"location":"#quick-start","title":"QUICK START","text":"<p>Install the requirements: </p> <pre><code>pip install -r requirements.txt \n</code></pre>"},{"location":"#load-a-sparse-autoencoder","title":"Load a Sparse Autoencoder","text":"<p>The following snippet shows how to load a Sparse Autoencoder from a wandb project. </p> <pre><code>from sentence_transformers import SentenceTransformer\nfrom config import get_default_cfg\nfrom transformers import pipeline\nfrom sae import JumpReLUSAE\nimport wandb\nimport torch\nimport json\nimport os\n\nsbert = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\ncfg = get_default_cfg()\n\nrun = wandb.init()\nartifact = run.use_artifact('path_to_your_artifact', type='model')\nartifact_dir = artifact.download()\nconfig_path = os.path.join(artifact_dir, 'config.json')\nwith open(config_path, 'r') as f:\n    config = json.load(f)\n\nif \"dtype\" in config and isinstance(config[\"dtype\"], str):\n    if config[\"dtype\"] == \"torch.float32\":\n        config[\"dtype\"] = torch.float32\n    elif config[\"dtype\"] == \"torch.float16\":\n        config[\"dtype\"] = torch.float16\n\nsae = JumpReLUSAE(config).to(config[\"device\"])\nsae.load_state_dict(torch.load(os.path.join(artifact_dir, 'sae.pt')))\n</code></pre> <p>Once the model is loaded, you can use it to obtain the features from a specific text. </p>"},{"location":"activationstore/","title":"Activation Store","text":"<p>Note: The code implemented in this section is based on the implementation  available here.  For this project, I have only modified it to adapt it to the <code>SentenceBERT</code> model. </p> <p>The <code>ActivationsStoreSBERT</code> class is a utility for efficiently extracting  and buffering dense <code>Sentence-BERT</code> embeddings from a streaming dataset.  It is used as an intermediate component between a sentence encoder and a  sparse autoencoder.</p>"},{"location":"activationstore/#purpose","title":"\ud83e\udde0 Purpose","text":"<p>Its main goal is to: - Stream examples from a large dataset. - Encode them using a Sentence-BERT model. - Buffer multiple batches of embeddings into memory. - Serve these embeddings as mini-batches for training or analysis.</p> <p>This is particularly useful when working with large-scale streaming datasets  (e.g. Hugging Face <code>load_dataset(..., streaming=True)</code>) that do not fit in memory.</p>"},{"location":"activationstore/#configuration-parameters","title":"\u2699\ufe0f Configuration Parameters","text":"Argument Description <code>model</code> A Sentence-BERT model with <code>.encode()</code> method (e.g. from <code>sentence-transformers</code>). <code>cfg</code> A dictionary containing the following keys: <code>dataset_path</code> Path to the Hugging Face dataset to load. <code>model_batch_size</code> Number of examples processed at once by the SBERT model. <code>device</code> <code>\"cuda\"</code> or <code>\"cpu\"</code> device for embedding extraction. <code>num_batches_in_buffer</code> Number of batches to pre-load and concatenate into an activation buffer. <code>num_examples</code> Total number of examples to extract. Acts as a cap to avoid exhausting memory."},{"location":"activationstore/#main-methods","title":"\ud83e\uddea Main Methods","text":""},{"location":"activationstore/#get_batch_tokens","title":"<code>get_batch_tokens()</code>","text":"<p>Streams a batch of raw text examples from the dataset. Raises an error if the dataset is exhausted or the example cap is reached.</p>"},{"location":"activationstore/#get_activationstexts","title":"<code>get_activations(texts)</code>","text":"<p>Encodes a list of texts into dense embeddings using the SBERT model.</p>"},{"location":"activationstore/#_fill_buffer","title":"<code>_fill_buffer()</code>","text":"<p>Fills an internal activation buffer with multiple batches of embeddings. This is used to simulate an in-memory dataset for later sampling.</p>"},{"location":"activationstore/#_get_dataloader","title":"<code>_get_dataloader()</code>","text":"<p>Creates a PyTorch <code>DataLoader</code> over the activation buffer for minibatch training or analysis.</p>"},{"location":"activationstore/#next_batch","title":"<code>next_batch()</code>","text":"<p>Returns the next minibatch of SBERT embeddings. If the internal buffer is exhausted, it is automatically refreshed by re-encoding new examples.</p>"},{"location":"activationstore/#lifecycle-and-usage","title":"\ud83d\udd01 Lifecycle and Usage","text":"<p>Typical usage pattern: ```python store = ActivationsStoreSBERT(model=sbert, cfg=config) batch = store.next_batch()  # returns a tensor of SBERT embeddings</p>"},{"location":"introduction/","title":"\ud83d\udcd8 Introduction","text":""},{"location":"introduction/#motivation","title":"\ud83e\udde0 Motivation","text":"<p>One of the main focus of mechanistic interpretability is to understand how language  models represent the information in their embeddings. The field has mostly focused on  studying the representations of the residual stream in autoregressive transformers.  However, little work has been done to understand the representations of other types  of language models, such as sentence embedding models. Till now, the only paper  that has studied the representations of sentence embedding models is  this one. This project will use the ideas shown in this paper to study the representations of <code>Sentence-BERT</code>, one of (if not the)  most popular sentence embedding model. More details about the model can be found  here. </p> <p>One of the biggest obstacles when training a Sparse Autoencoder (SAE), especially  when having a limited budget, is the amount of compute needed. A SAE is able to  extract monosemantic features from a specific part of the model, and it can't be used  in other parts. Because of this, one would need to train a SAE for each part of the  model, which would need a huge amount of computation and training time. However, if  we study a sentence embedding model, there is no need to train a SAE for each part of  the model. Instead, we can train a SAE on the final embeddings and get a monosemantic  representation of the embedding space used to encode the sentences. </p> <p>Once the SAE is trained, the features must be interpreted. A SAE usually has lots of  features, making it not possible to analyze each one of them manually. Because of that,  many efforts have been made to develop methods to interpret the features obtained from the SAEs. The most common method is to call a LLM to generate a description of the  feature. However, making a huge number of API calls to a LLM is costly,  which makes it not possible to use this method in a limited budget. This project  presents a new feature interpretation method that does not require any API calls. It is based on keyword extraction via <code>Key-BERT</code>. </p>"},{"location":"introduction/#project-objectives","title":"\ud83c\udfaf Project Objectives","text":"<p>This project aims to explore the use of Sparse Autoencoders for interpreting  <code>Sentence-BERT</code> embeddings. The main goals are: </p> <ul> <li>Train a SAE on <code>Sentence-BERT</code> embeddings from different knowledge domains. </li> <li>Present a method to interpret the features obtained from the SAEs. </li> </ul>"},{"location":"introduction/#scope","title":"\ud83d\udd0d Scope","text":"<p>This work does not aim to improve downstream task performance, nor to benchmark  new autoencoder architectures. Instead, it focuses on applying the existing  architectures to a new domain and developing a method to interpret the features. </p> <p>The entire pipeline is modular and open-source, and can be adapted to other embedding  models or datasets.</p>"},{"location":"introduction/#documentation-structure","title":"\ud83e\udded Documentation Structure","text":"<p>The documentation is organized as follows:</p> <ul> <li>Quick start: A minimal guide to set up the environment and run the pipeline.</li> <li>Introduction to the project: Motivation, objectives, and context for this research.</li> <li>SAE models: Overview of the different Sparse Autoencoder architectures used (Vanilla, TopK, JumpReLU).</li> <li>Activation Store: Details of the component responsible for extracting and buffering SBERT embeddings.</li> <li>Training: Description of the training pipeline and configuration options for SAE models.</li> <li>Feature analysis:</li> <li>Keyword extraction: Methods to extract representative words for each latent feature.</li> <li>Visualization: Techniques to explore the feature space, activation patterns, and semantic alignment.</li> <li>Results: Qualitative and quantitative insights from experiments across different datasets and architectures.</li> </ul> <p>This documentation is intended to be modular and accessible. Each section can be read independently depending on your interest\u2014whether you're focused on model training, interpretability methods, or practical application.</p>"},{"location":"references/","title":"\ud83d\udcda Academic References","text":"<p>This project builds upon a variety of academic and technical sources related to  sparse autoencoders, interpretability, and language modeling.  The following references were used primarily in the theoretical framework of the  thesis:</p> <ul> <li>Anthropic Interpretability Team. (Jan 2025). Circuits updates. Retrieved from Transformer Circuits Thread: https://transformer-circuits.pub/2025/january-update/index.html  </li> <li>Bloom, J. (Feb 2, 2024). Open Source Sparse Autoencoders for all Residual Stream Layers of GPT2-Small. Retrieved from LessWrong: https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream  </li> <li>Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., ... Olah, C. (Oct 4, 2023). Towards Monosemanticity: Decomposing Language Models with Dictionary Learning. Retrieved from: https://transformer-circuits.pub/2023/monosemantic-features/index.html  </li> <li>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., &amp; Amodei, D. (2020). Language models are few-shot learners. Advances in Neural Information Processing Systems, 33.  </li> <li>Bussmann, B., Leask, P., &amp; Nanda, N. (2024). BatchTopK Sparse Autoencoders.  </li> <li>Chaudhary, M., &amp; Geiger, A. (2024). Evaluating open-source sparse autoencoders on disentangling factual knowledge in GPT-2 small. arXiv preprint.  </li> <li>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT (Vol. 1).  </li> <li>Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., ... Olah, C. (Sept 14, 2022). Toy Models of Superposition. Retrieved from: https://transformer-circuits.pub/2022/toy_model/index.html  </li> <li>Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., ... Olah, C. (Dec 22, 2021). A Mathematical Framework for Transformer Circuits. Retrieved from: https://transformer-circuits.pub/2021/framework/index.html  </li> <li>Gao, L., Dupr\u00e9 de la Tour, T., Tillman, H., Goh, G., Troll, R., Radford, A., ... Wu, J. (2024). Scaling and Evaluating Sparse Autoencoders.  </li> <li>Li, Y., Ildiz, M. E., Papailiopoulos, D., &amp; Oymak, S. (2023). Transformers as Algorithms: Generalization and Stability in In-Context Learning. In International Conference on Machine Learning (ICML).  </li> <li>Mikolov, T., Yih, W.-t., &amp; Zweig, G. (2013). Linguistic Regularities in Continuous Space Word Representations. In NAACL-HLT, pp. 746\u2013751.  </li> <li>O'Neill, C., Ye, C., Iyer, K., &amp; Wu, J. F. (2024). Disentangling Dense Embeddings with Sparse Autoencoders. arXiv preprint.  </li> <li>Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). Improving Language Understanding by Generative Pre-Training.  </li> <li>Rajamanoharan, S., Conmy, A., Smith, L., Lieberum, T., Varma, V., Kram\u00e1r, J., ... Nanda, N. (2024). Improving Dictionary Learning with Gated Sparse Autoencoders. arXiv preprint.  </li> <li>Rajamanoharan, S., Kram\u00e1r, J., &amp; Nanda, N. (2024). Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders.  </li> <li>Reimers, N., &amp; Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint.  </li> <li>Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., ... Henighan, T. (May 21, 2024). Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet. Retrieved from: https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html  </li> <li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &amp; Polosukhin, I. (2017). Attention is All You Need. Advances in Neural Information Processing Systems, 30.  </li> <li>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E. H., Narang, S., ... Zhou, D. (2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv preprint.  </li> <li>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., &amp; Zhou, D. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS, 35.  </li> <li>Zhou, D., Sch\u00e4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., ... Chi, E. (2022). Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. arXiv preprint.  </li> </ul>"},{"location":"saes/","title":"SAE models","text":"<p>Note  I did not develop The SAE models used in this project from scratch.  They are extracted from an existing implementation available at:  https://github.com/bartbussmann/BatchTopK.  The focus of this project is not the implementation and training of the SAEs, but the developement  of a method for interpreting the features obtained from them. </p> <p>There are a total of four different SAE implementations: </p> <ul> <li><code>VanillaSAE</code>: Original SAE implementation. </li> <li><code>JumpReLU</code>: SAE with <code>JumpReLU</code> activation function. </li> <li><code>BatchTopK</code>: SAE with <code>BatchTopK</code> activation function. </li> <li><code>BatchTopKJumpReLU</code>: SAE with <code>BatchTopK</code> for training and <code>JumpReLU</code> for inference.</li> </ul> <p>For more information about the SAEs, please refer to the original repository. </p>"},{"location":"training/","title":"\ud83d\udd27 SAE Training Script","text":"<p>This script defines the training pipeline for various Sparse Autoencoder (SAE) architectures over Sentence-BERT embeddings. It allows configuration of the SAE variant, dataset, training parameters, and model behavior.</p>"},{"location":"training/#overview","title":"\ud83d\udce6 Overview","text":"<p>The script performs the following steps:</p> <ol> <li>Load a pre-trained Sentence-BERT model.</li> <li>Stream text data from a dataset.</li> <li>Encode the texts into dense embeddings.</li> <li>Train a selected SAE variant on these embeddings.</li> </ol>"},{"location":"training/#configuration-get_experiment_cfg","title":"\u2699\ufe0f Configuration: <code>get_experiment_cfg()</code>","text":"<p>The function <code>get_experiment_cfg()</code> sets up the experiment configuration.  It returns a dictionary (<code>cfg</code>) with training hyperparameters and system settings:</p> Key Description <code>sae_type</code> SAE variant to use (<code>vanilla</code>, <code>topk</code>, <code>batchtopk</code>, <code>jumprelu</code>) <code>model_name</code> Pre-trained SBERT model name (Hugging Face) <code>dataset_path</code> Hugging Face dataset path (streamed) <code>dict_size</code> Number of latent features (SAE output dimensionality) <code>top_k</code> Number of active features allowed in the SAE output <code>lr</code> Learning rate <code>aux_penalty</code> Auxiliary penalty (e.g., JumpReLU regularization) <code>input_unit_norm</code> Whether to normalize input embeddings <code>device</code> <code>\"cuda\"</code> or <code>\"cpu\"</code> <code>wandb_project</code> Project name for Weights &amp; Biases logging <p>The config is finalized by <code>post_init_cfg(cfg)</code>, which fills in  derived or default values.</p>"},{"location":"training/#sae-selection","title":"\ud83e\udde0 SAE Selection","text":"<p>The script dynamically selects the appropriate SAE class:</p> <pre><code>sae_classes = {\n    \"vanilla\": VanillaSAE,\n    \"topk\": TopKSAE,\n    \"batchtopk\": BatchTopKSAE,\n    \"jumprelu\": JumpReLUSAE,\n}\nsae = sae_classes[cfg[\"sae_type\"]](cfg)\n</code></pre> <p>Each SAE receives the same configuration dictionary.</p>"},{"location":"training/#data-loading-activationsstoresbert","title":"\ud83d\udcca Data Loading: <code>ActivationsStoreSBERT</code>","text":"<p>The class <code>ActivationsStoreSBERT</code> is responsible for efficiently streaming and  buffering text data as <code>Sentence-BERT</code> embeddings.</p> <p>It performs the following tasks:</p> <ul> <li>Streams raw text from a Hugging Face dataset using <code>streaming=True</code>.</li> <li>Encodes text batches using a pre-trained Sentence-BERT model.</li> <li>Buffers multiple batches into memory to simulate a fixed-size dataset.</li> <li>Provides minibatches of embeddings for training or analysis.</li> </ul> <pre><code>activation_store = ActivationsStoreSBERT(model, cfg)\n</code></pre> <p>This design enables large-scale training without loading the entire dataset into memory. It also handles iteration limits and buffer regeneration automatically.</p>"},{"location":"tutorials/feature_analysis/","title":"Feature Analysis","text":"<p>This module provides tools for discovering and interpreting feature families and semantic clusters  from the Sparse Autoencoder's latent space.</p>"},{"location":"tutorials/feature_analysis/#modules","title":"Modules","text":"<ul> <li><code>coactivation_graph</code>: Build feature graphs based on coactivations.</li> <li><code>family_evaluation</code>: Evaluate structural quality of extracted families.</li> <li><code>feature_clustering</code>: Cluster features based on keyword embeddings.</li> <li><code>cluster_diagnostics</code>: Interpret and diagnose feature clusters.</li> <li><code>visualization</code>: Plot UMAP projections and coactivation heatmaps.</li> <li><code>utils</code>: Auxiliary functions (e.g., matrix normalization, JSON I/O).</li> </ul>"},{"location":"tutorials/feature_analysis/#workflow","title":"Workflow","text":"<pre><code># 1. Load the coactivation matrix\nC = np.load(\"coactivation_matrix_csLG.npy\")\n</code></pre> <pre><code># 2. Build coactivation graph\nfrom sae_feature_analysis import coactivation_graph\n\nG = coactivation_graph.build_coactivation_graph(C)\nDG = coactivation_graph.build_max_spanning_digraph(G, np.diag(C))\n</code></pre> <pre><code># 3. Extract families\nfamilies = coactivation_graph.extract_feature_families(DG)\n</code></pre> <pre><code># 4. Evaluate families\nfrom sae_feature_analysis import family_evaluation\n\nmetrics = family_evaluation.evaluate_feature_families(C, families)\ntop_families = family_evaluation.sort_families_by_block_ratio(metrics)\n</code></pre> <pre><code># 5. Load keywords and cluster features\nfrom sae_feature_analysis import feature_clustering\n\nkeywords = load_json(\"feature_keywords.json\")\nembeddings = feature_clustering.compute_weighted_feature_embeddings(keywords)\nlabels = feature_clustering.cluster_embeddings(embeddings)\n</code></pre> <pre><code># 6. Plot UMAP and heatmaps\nfrom sae_feature_analysis import visualization\n\nvisualization.plot_umap_scatter(embeddings, labels)\n</code></pre>"},{"location":"tutorials/keywords/","title":"Keyword extraction for fature descriptions","text":"<p>As stated in the introduction, the main focus of this project is to develop a method  for interpreting the features obtained from a Sparse Autoencoder. The proposed method  is based on <code>KeyBERT</code>, a keyword extraction  library availiable in Python. The idea is to extract the most relevant words from each  top-k activating text, and implement a scoring system to rank the terms. This tutorial  will show an example of how to implement this method. For more details on the  theoretical background, please refer to this section of the documentation. </p> <p>You can use the following code to get the keywords for each feature. </p> <pre><code>from keybert import KeyBERT\nfrom sentence_transformers import SentenceTransformer, util\nfrom collections import defaultdict\nfrom itertools import combinations\nfrom math import log\n\nkw_model = KeyBERT()\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\nSIMILARITY_THRESHOLD = 0.9\nBIGRAM_SIM_THRESHOLD = 0.4\nGROUP_SIM_THRESHOLD = 0.75\n\nall_keywords_results = []\n\nfor feature_id in range(len(top_activations)):\n    keyword_sums = defaultdict(float)\n    keyword_counts = defaultdict(int)\n    original_forms = defaultdict(list)\n\n    for i in range(len(top_activations[feature_id])):\n        text = top_activations[feature_id][i][1]\n        text_embedding = embedding_model.encode(text, convert_to_tensor=True)\n\n        keywords = kw_model.extract_keywords(text)\n        keywords = [(w.lower(), s) for w, s in keywords]\n        if not keywords:\n            continue\n\n        words, scores = zip(*keywords)\n        embeddings = embedding_model.encode(words, convert_to_tensor=True)\n\n        grouped_indices = set()\n        local_groups = []\n        for i, (word_i, coef_i) in enumerate(keywords):\n            if i in grouped_indices:\n                continue\n            group = [(word_i, coef_i)]\n            grouped_indices.add(i)\n            for j in range(i + 1, len(keywords)):\n                if j in grouped_indices:\n                    continue\n                sim = util.cos_sim(embeddings[i], embeddings[j]).item()\n                if sim &gt;= SIMILARITY_THRESHOLD:\n                    group.append((keywords[j][0], keywords[j][1]))\n                    grouped_indices.add(j)\n            local_groups.append(group)\n\n        compact_keywords = []\n        for group in local_groups:\n            representative, score = max(group, key=lambda x: x[1])\n            compact_keywords.append((representative, score))\n\n        top_keywords = compact_keywords[:5]\n        top_words = [w for w, _ in top_keywords]\n        for w1, w2 in combinations(top_words, 2):\n            phrase = f\"{w1} {w2}\"\n            phrase_embedding = embedding_model.encode(phrase, convert_to_tensor=True)\n            sim_to_text = util.cos_sim(phrase_embedding, text_embedding).item()\n            if sim_to_text &gt;= BIGRAM_SIM_THRESHOLD:\n                compact_keywords.append((phrase, sim_to_text))\n\n        seen_in_this_doc = set()\n        for word, coef in compact_keywords:\n            keyword_sums[word] += coef\n            original_forms[word].append((word, coef))\n            if word not in seen_in_this_doc:\n                keyword_counts[word] += 1\n                seen_in_this_doc.add(word)\n\n    scored_keywords = {\n        word: (keyword_sums[word] / keyword_counts[word]) * log(1 + keyword_counts[word])\n        for word in keyword_sums\n    }\n    sorted_keywords = sorted(scored_keywords.items(), key=lambda x: x[1], reverse=True)\n\n    terms = [term for term, score in sorted_keywords]\n    if terms:\n        term_embeddings = embedding_model.encode(terms, convert_to_tensor=True)\n    else:\n        term_embeddings = None\n\n    clustered = []\n    assigned = set()\n    for i, term in enumerate(terms):\n        if term in assigned:\n            continue\n        cluster = [term]\n        assigned.add(term)\n        for j in range(i + 1, len(terms)):\n            if terms[j] in assigned:\n                continue\n            sim = util.cos_sim(term_embeddings[i], term_embeddings[j]).item()\n            if sim &gt;= GROUP_SIM_THRESHOLD:\n                cluster.append(terms[j])\n                assigned.add(terms[j])\n        clustered.append(cluster)\n\n    aggregated_keywords = []\n    for cluster in clustered:\n        aggregated_S = sum(keyword_sums[w] for w in cluster)\n        aggregated_N = sum(keyword_counts[w] for w in cluster)\n        if aggregated_N &gt; 0:\n            agg_score = (aggregated_S / aggregated_N) * log(1 + aggregated_N)\n            rep = max(cluster, key=lambda w: scored_keywords.get(w, 0))\n            aggregated_keywords.append((rep, agg_score))\n\n    sorted_aggregated_keywords = sorted(aggregated_keywords, key=lambda x: x[1], reverse=True)\n\n    all_keywords_results.append({\n        \"feature_id\": feature_id,\n        \"keywords\": sorted_aggregated_keywords\n    })\n\n    if (feature_id + 1) % 500 == 0:\n        print(f\"Processed {feature_id + 1} features...\")\n\n    temp = get_gpu_temperature()\n    if temp is not None and temp &gt;= 77:\n        wait_for_gpu_cooling(threshold=77, resume_temp=65, check_interval=5)\n</code></pre> <p>For example, if your SAE is trained on Machine Learning texts and it had a RL feature,  the keywords for that feature could look like this: </p> <pre><code>{'feature_id': 2220,\n 'keywords': [['reinforcement', 1.620540681324779],\n  ['reinforcement learning', 0.9814733623507801],\n  ['reward optimization', 0.9179529960192596]]}\n</code></pre>"},{"location":"tutorials/tutorial_use/","title":"How to use a SAE for analyzing a dataset","text":"<p>Once you have trained a SAE and saved it as an artifact in your Weights &amp; Biases project,  you can use it to analyze a dataset. This section will guide you through the process. </p>"},{"location":"tutorials/tutorial_use/#load-the-sae-artifact","title":"\ud83d\udcf2 Load the SAE artifact","text":"<p>First, import the necessary libraries and load the SAE artifact.</p> <pre><code>from sentence_transformers import SentenceTransformer\nfrom transformers import pipeline\nfrom config import get_default_cfg\nfrom sae import JumpReLUSAE\nimport wandb\nimport os\nimport json\nimport torch\nimport numpy as np \n\nsbert = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\ndistilbert = pipeline(\"fill-mask\", model=\"distilbert/distilbert-base-cased\")\ncfg = get_default_cfg()\n\nrun = wandb.init()\nartifact = run.use_artifact('ybiku-unir/SBERT-SAEs-csLG/sentence-transformers_paraphrase-mpnet-base-v2_blocks.0.hook_embed_2304_jumprelu_16_0.0003_389:v0', type='model')\nartifact_dir = artifact.download()\nconfig_path = os.path.join(artifact_dir, 'config.json')\nwith open(config_path, 'r') as f:\n    config = json.load(f)\n\nif \"dtype\" in config and isinstance(config[\"dtype\"], str):\n    if config[\"dtype\"] == \"torch.float32\":\n        config[\"dtype\"] = torch.float32\n    elif config[\"dtype\"] == \"torch.float16\":\n        config[\"dtype\"] = torch.float16\n\nsae = JumpReLUSAE(config).to(config[\"device\"])\nsae.load_state_dict(torch.load(os.path.join(artifact_dir, 'sae.pt')))\n</code></pre> <p>The script above will use the configuration stored in the artifact to load the model. This way, you will not need to worry about defining the model architecture again. </p>"},{"location":"tutorials/tutorial_use/#load-the-dataset","title":"\ud83d\udcca Load the dataset","text":"<p>Next, you need to create the dataset you want to analyze. It needs to be a custom class  which inherits from the <code>IterableDataset</code> class. The following snippet shows an example  of how to do it: </p> <pre><code>from torch.utils.data import DataLoader, IterableDataset\nfrom datasets import load_dataset\n\nclass HFDatasetWrapper(IterableDataset):\n    def __init__(self, hf_dataset):\n        self.hf_dataset = hf_dataset\n\n    def __iter__(self):\n        for item in self.hf_dataset:\n            if item is not None and item.get(\"text\"):\n                yield item\n\ndef collate_fn_skip_none(batch):\n    return [item for item in batch if item is not None]\n\nhf_dataset = load_dataset(\"UniverseTBD/arxiv-astro-abstracts-all\", split=\"train\", streaming=True)\ndataset = HFDatasetWrapper(hf_dataset)\n\nnum_examples = config[\"num_examples\"]\ndevice = config[\"device\"]\ndict_size = config[\"dict_size\"]\n\nsbert = sbert.to(device)\nsae = sae.to(device)\nsae.eval()\n\ndataloader = DataLoader(dataset, batch_size=64, collate_fn=collate_fn_skip_none)\n</code></pre> <p>With this code, you are now ready to analyze the dataset. </p>"},{"location":"tutorials/tutorial_use/#get-feature-density-histogram","title":"\ud83d\udd0d Get feature density histogram","text":"<p>One of the indicators of whether the SAE has a good sparsity is the feature density histogram. To get the feature activations, you can run the following snippet: </p> <pre><code>feature_count = torch.zeros(dict_size, device=device)\nprocessed = 0\n\nfor batch in dataloader:\n    if processed &gt;= num_examples:\n        break\n\n    texts = [item[\"text\"] for item in batch]\n    embeddings = sbert.encode(texts, convert_to_tensor=True, device=device)\n\n    with torch.no_grad():\n        sae_out = sae(embeddings)\n\n    feature_acts = sae_out[\"feature_acts\"]\n    batch_count = (feature_acts &gt; 0).float().sum(dim=0)\n    feature_count += batch_count\n    processed += len(texts)\n\n    if processed % 20000 == 0:\n        print(f\"[INFO] {processed} examples processed.\")\n\n    temp = get_gpu_temperature()\n    if temp is not None and temp &gt;= 74:\n        wait_for_gpu_cooling(threshold=74, resume_temp=60, check_interval=10)\n\nfeature_fire_rate = 100 * feature_count / num_examples\nnp.save(\"fire_rate_astro.npy\", feature_fire_rate.cpu().numpy())\n</code></pre> <p>the <code>.npy</code> file will contain the number of times each feature was activated. If you  want to plot the histogram, you can use the following code: </p> <pre><code>import matplotlib.pyplot as plt \nlog_feature_fire_rate = torch.log10((feature_fire_rate / 100) + 1e-10).cpu().numpy()\n\nplt.style.use('default')\nplt.hist(log_feature_fire_rate, bins=50, color='tab:blue')\nplt.xlabel(\"Log10 Feature density\")\nplt.ylabel(\"Number of features\")\nplt.title(\"Log Feature Fire Rate Distribution (csLG)\")\nplt.xlim(-10, 0)\nplt.show()\n</code></pre> <p>Features with a 0% fire rate will be placed in the -10 value in the x-axis, and  features with a 100% fire rate will be placed in the 0 value. For further details on  how to interpret the histogram, I highly recommend reading  this post. </p>"},{"location":"tutorials/tutorial_use/#get-the-top-10-activating-texts","title":"\ud83d\udcd1 Get the top-10 activating texts","text":"<p>To generate the descriptions for the features, first you need to get examples of the  texts which most activated each feature. You can do this by running the following code: </p> <pre><code>import heapq\n\nnum_features = config[\"dict_size\"]\ntop_activations = [[] for _ in range(num_features)]\n\nfor i, example in enumerate(dataset):\n    if i &gt;= num_examples: break\n    text = example[\"abstr\"]\n    embedding = sbert.encode(text, convert_to_tensor=True).squeeze(0).to(config[\"device\"])\n    with torch.no_grad():\n        sae_out = sae(embedding)\n    feature_acts = sae_out[\"feature_acts\"]\n\n    for j in range(num_features):\n        activation_value = feature_acts[j].item()\n        heap = top_activations[j]\n        if len(heap) &lt; 10: # Change this to the number of examples you want \n            heapq.heappush(heap, (activation_value, text))\n        else:\n            heapq.heappushpop(heap, (activation_value, text))\n\n    if i % 5000 == 0:\n        print(f\"Processed {i} examples\")\n\n\ntop_activations = [sorted(heap, key=lambda x: x[0], reverse=True) for heap in top_activations]\nwith open(\"top_activations_astro.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(top_activations, f, indent=2, ensure_ascii=False)\n</code></pre> <p>Once you have the top-k activating texts, you are now redy to generate the descriptions. </p>"}]}