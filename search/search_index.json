{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quick start","text":"<p>\u26a0\ufe0fWARNING  This page is still under construction. </p> <p>This codebase exists to provide a simple environment for:</p> <ul> <li>Training Sparse Autoencoders (SAEs) on Sentence-BERT embeddings. </li> <li>Analyzing the features obtained from the SAEs. </li> </ul>"},{"location":"#quick-start","title":"QUICK START","text":"<p>Install the requirements: </p> <pre><code>pip install -r requirements.txt \n</code></pre>"},{"location":"#load-a-sparse-autoencoder","title":"Load a Sparse Autoencoder","text":"<p>The following snippet shows how to load a Sparse Autoencoder from a wandb project. </p> <pre><code>from config import get_default_cfg \nfrom explainer import Explainer\n\ncfg = get_default_cfg()\ncfg[\"artifact_path\"] = 'your_artifact_path' # Path to the artifact in wandb \n\nexplainer = Explainer(cfg)\nsae = explainer.load_sae()\n</code></pre>"},{"location":"#sparse-autoencoder-analysis","title":"Sparse Autoencoder Analysis","text":"<p>The following snippet shows how to analyze the features obtained from the Sparse Autoencoder. You can  compute the fire rate of each feature, get the top activating texts for each feature and extract the  keywords which describe them. </p> <pre><code>fire_rate = explainer.compute_fire_rate(column=\"text\", save_path=\"fire_rate.npy\")\ntop_activations = explainer.get_top_activating_texts(\n    num_examples = 100,\n    top_k = 2,\n    save_to_json = True\n)\nkeywords = explainer.extract_keywords_per_feature(\n    top_activations, top_n_keywords = 3\n)\n</code></pre> <p><code>top_activations</code> is a dictionary, each key corresponding to a feature. Inside each key, there is a list  of tuples of the form <code>[(activation), text]</code>. <code>activation</code> is the activation produced by the text to the  feature, and <code>text</code> is the text itself. For example, a possible output of <code>top_activations</code> could be: </p> <pre><code>[[2.9942984580993652,\n  '  Trust region policy optimization (TRPO) is a popular and empirically\\nsuccessful policy search \n  algorithm in Reinforcement Learning (RL) in which a\\nsurrogate problem, that restricts consecutive \n  policies to be \\'close\\' to one\\nanother, is iteratively solved. Nevertheless, TRPO has been considered \n  a\\nheuristic algorithm inspired by Conservative Policy Iteration (CPI). We show\\nthat the adaptive \n  scaling mechanism used in TRPO is in fact the natural \"RL\\nversion\" of traditional trust-region methods \n  from convex analysis. We first\\nanalyze TRPO in the planning setting, in which we have access to the \n  model and\\nthe entire state space. Then, we consider sample-based TRPO and \n  establish\\n$\\\\tilde O(1/\\\\sqrt{N})$ convergence rate to the global optimum. Importantly, \n  the\\nadaptive scaling mechanism allows us to analyze TRPO in regularized MDPs for\\nwhich we prove fast \n  rates of $\\\\tilde O(1/N)$, much like results in convex\\noptimization. This is the first result in RL \n  of better rates when regularizing\\nthe instantaneous cost or reward.\\n'],\n [2.9936885833740234,\n  '  We present a reinforcement learning (RL) framework to synthesize a control\\npolicy from a given linear\n   temporal logic (LTL) specification in an unknown\\nstochastic environment that can be modeled as a Markov\n    Decision Process (MDP).\\nSpecifically, we learn a policy that maximizes the probability of \n    satisfying\\nthe LTL formula without learning the transition probabilities. We introduce a\\nnovel \n    rewarding and path-dependent discounting mechanism based on the LTL\\nformula such that (i) an optimal \n    policy maximizing the total discounted reward\\neffectively maximizes the probabilities of satisfying \n    LTL objectives, and (ii)\\na model-free RL algorithm using these rewards and discount factors \n    is\\nguaranteed to converge to such policy. Finally, we illustrate the applicability\\nof our RL-based \n    synthesis approach on two motion planning case studies.\\n']]\n</code></pre> <p><code>keywords</code> has a similar structure. It is also a dictionary, each key corresponding to a feature.  Inside each key, there is a list of tuples of the form <code>[(keyword), score]</code>. <code>keyword</code> is the keyword itself,  and <code>score</code> is the score of the keyword. For example, a possible output of <code>keywords</code> could be: </p> <pre><code>{'feature_id': 2220,\n 'keywords': [['reinforcement', 1.620540681324779],\n  ['reinforcement learning', 0.9814733623507801],\n  ['reward optimization', 0.9179529960192596]} \n</code></pre>"},{"location":"activationstore/","title":"Activation Store","text":"<p>Note: The code implemented in this section is based on the implementation  available here.  For this project, I have only modified it to adapt it to the <code>SentenceBERT</code> model. </p> <p>The <code>ActivationsStoreSBERT</code> class is a utility for efficiently extracting  and buffering dense <code>Sentence-BERT</code> embeddings from a streaming dataset.  It is used as an intermediate component between a sentence encoder and a  sparse autoencoder.</p>"},{"location":"activationstore/#purpose","title":"\ud83e\udde0 Purpose","text":"<p>Its main goal is to: - Stream examples from a large dataset. - Encode them using a Sentence-BERT model. - Buffer multiple batches of embeddings into memory. - Serve these embeddings as mini-batches for training or analysis.</p> <p>This is particularly useful when working with large-scale streaming datasets  (e.g. Hugging Face <code>load_dataset(..., streaming=True)</code>) that do not fit in memory.</p>"},{"location":"activationstore/#configuration-parameters","title":"\u2699\ufe0f Configuration Parameters","text":"Argument Description <code>model</code> A Sentence-BERT model with <code>.encode()</code> method (e.g. from <code>sentence-transformers</code>). <code>cfg</code> A dictionary containing the following keys: <code>dataset_path</code> Path to the Hugging Face dataset to load. <code>model_batch_size</code> Number of examples processed at once by the SBERT model. <code>device</code> <code>\"cuda\"</code> or <code>\"cpu\"</code> device for embedding extraction. <code>num_batches_in_buffer</code> Number of batches to pre-load and concatenate into an activation buffer. <code>num_examples</code> Total number of examples to extract. Acts as a cap to avoid exhausting memory."},{"location":"activationstore/#main-methods","title":"\ud83e\uddea Main Methods","text":""},{"location":"activationstore/#get_batch_tokens","title":"<code>get_batch_tokens()</code>","text":"<p>Streams a batch of raw text examples from the dataset. Raises an error if the dataset is exhausted or the example cap is reached.</p>"},{"location":"activationstore/#get_activationstexts","title":"<code>get_activations(texts)</code>","text":"<p>Encodes a list of texts into dense embeddings using the SBERT model.</p>"},{"location":"activationstore/#_fill_buffer","title":"<code>_fill_buffer()</code>","text":"<p>Fills an internal activation buffer with multiple batches of embeddings. This is used to simulate an in-memory dataset for later sampling.</p>"},{"location":"activationstore/#_get_dataloader","title":"<code>_get_dataloader()</code>","text":"<p>Creates a PyTorch <code>DataLoader</code> over the activation buffer for minibatch training or analysis.</p>"},{"location":"activationstore/#next_batch","title":"<code>next_batch()</code>","text":"<p>Returns the next minibatch of SBERT embeddings. If the internal buffer is exhausted, it is automatically refreshed by re-encoding new examples.</p>"},{"location":"activationstore/#lifecycle-and-usage","title":"\ud83d\udd01 Lifecycle and Usage","text":"<p>Typical usage pattern: ```python store = ActivationsStoreSBERT(model=sbert, cfg=config) batch = store.next_batch()  # returns a tensor of SBERT embeddings</p>"},{"location":"introduction/","title":"\ud83d\udcd8 Introduction","text":""},{"location":"introduction/#motivation","title":"\ud83e\udde0 Motivation","text":"<p>One of the main focus of mechanistic interpretability is to understand how language  models represent the information in their embeddings. The field has mostly focused on  studying the representations of the residual stream in autoregressive transformers.  However, little work has been done to understand the representations of other types  of language models, such as sentence embedding models. Till now, the only paper  that has studied the representations of sentence embedding models is  this one. This project will use the ideas shown in this paper to study the representations of <code>Sentence-BERT</code>, one of (if not the)  most popular sentence embedding model. More details about the model can be found  here. </p> <p>One of the biggest obstacles when training a Sparse Autoencoder (SAE), especially  when having a limited budget, is the amount of compute needed. A SAE is able to  extract monosemantic features from a specific part of the model, and it can't be used  in other parts. Because of this, one would need to train a SAE for each part of the  model, which would need a huge amount of computation and training time. However, if  we study a sentence embedding model, there is no need to train a SAE for each part of  the model. Instead, we can train a SAE on the final embeddings and get a monosemantic  representation of the embedding space used to encode the sentences. </p> <p>Once the SAE is trained, the features must be interpreted. A SAE usually has lots of  features, making it not possible to analyze each one of them manually. Because of that,  many efforts have been made to develop methods to interpret the features obtained from the SAEs. The most common method is to call a LLM to generate a description of the  feature. However, making a huge number of API calls to a LLM is costly,  which makes it not possible to use this method in a limited budget. This project  presents a new feature interpretation method that does not require any API calls. It is based on keyword extraction via <code>Key-BERT</code>. </p>"},{"location":"introduction/#project-objectives","title":"\ud83c\udfaf Project Objectives","text":"<p>This project aims to explore the use of Sparse Autoencoders for interpreting  <code>Sentence-BERT</code> embeddings. The main goals are: </p> <ul> <li>Train a SAE on <code>Sentence-BERT</code> embeddings from different knowledge domains. </li> <li>Present a method to interpret the features obtained from the SAEs. </li> </ul>"},{"location":"introduction/#scope","title":"\ud83d\udd0d Scope","text":"<p>This work does not aim to improve downstream task performance, nor to benchmark  new autoencoder architectures. Instead, it focuses on applying the existing  architectures to a new domain and developing a method to interpret the features. </p> <p>The entire pipeline is modular and open-source, and can be adapted to other embedding  models or datasets.</p>"},{"location":"introduction/#documentation-structure","title":"\ud83e\udded Documentation Structure","text":"<p>The documentation is organized as follows:</p> <ul> <li>Quick start: A minimal guide to set up the environment and run the pipeline.</li> <li>Introduction to the project: Motivation, objectives, and context for this research.</li> <li>SAE models: Overview of the different Sparse Autoencoder architectures used (Vanilla, TopK, JumpReLU).</li> <li>Activation Store: Details of the component responsible for extracting and buffering SBERT embeddings.</li> <li>Training: Description of the training pipeline and configuration options for SAE models.</li> <li>Feature analysis:</li> <li>Keyword extraction: Methods to extract representative words for each latent feature.</li> <li>Visualization: Techniques to explore the feature space, activation patterns, and semantic alignment.</li> <li>Results: Qualitative and quantitative insights from experiments across different datasets and architectures.</li> </ul> <p>This documentation is intended to be modular and accessible. Each section can be read independently depending on your interest\u2014whether you're focused on model training, interpretability methods, or practical application.</p>"},{"location":"saes/","title":"SAE models","text":"<p>Note  I did not develop The SAE models used in this project from scratch.  They are extracted from an existing implementation available at:  https://github.com/bartbussmann/BatchTopK.  The focus of this project is not the implementation and training of the SAEs, but the developement  of a method for interpreting the features obtained from them. </p> <p>There are a total of four different SAE implementations: </p> <ul> <li><code>VanillaSAE</code>: Original SAE implementation. </li> <li><code>JumpReLU</code>: SAE with <code>JumpReLU</code> activation function. </li> <li><code>BatchTopK</code>: SAE with <code>BatchTopK</code> activation function. </li> <li><code>BatchTopKJumpReLU</code>: SAE with <code>BatchTopK</code> for training and <code>JumpReLU</code> for inference.</li> </ul> <p>For more information about the SAEs, please refer to the original repository. </p>"},{"location":"training/","title":"\ud83d\udd27 SAE Training Script","text":"<p>This script defines the training pipeline for various Sparse Autoencoder (SAE) architectures over Sentence-BERT embeddings. It allows configuration of the SAE variant, dataset, training parameters, and model behavior.</p>"},{"location":"training/#overview","title":"\ud83d\udce6 Overview","text":"<p>The script performs the following steps:</p> <ol> <li>Load a pre-trained Sentence-BERT model.</li> <li>Stream text data from a dataset.</li> <li>Encode the texts into dense embeddings.</li> <li>Train a selected SAE variant on these embeddings.</li> </ol>"},{"location":"training/#configuration-get_experiment_cfg","title":"\u2699\ufe0f Configuration: <code>get_experiment_cfg()</code>","text":"<p>The function <code>get_experiment_cfg()</code> sets up the experiment configuration.  It returns a dictionary (<code>cfg</code>) with training hyperparameters and system settings:</p> Key Description <code>sae_type</code> SAE variant to use (<code>vanilla</code>, <code>topk</code>, <code>batchtopk</code>, <code>jumprelu</code>) <code>model_name</code> Pre-trained SBERT model name (Hugging Face) <code>dataset_path</code> Hugging Face dataset path (streamed) <code>dict_size</code> Number of latent features (SAE output dimensionality) <code>top_k</code> Number of active features allowed in the SAE output <code>lr</code> Learning rate <code>aux_penalty</code> Auxiliary penalty (e.g., JumpReLU regularization) <code>input_unit_norm</code> Whether to normalize input embeddings <code>device</code> <code>\"cuda\"</code> or <code>\"cpu\"</code> <code>wandb_project</code> Project name for Weights &amp; Biases logging <p>The config is finalized by <code>post_init_cfg(cfg)</code>, which fills in  derived or default values.</p>"},{"location":"training/#sae-selection","title":"\ud83e\udde0 SAE Selection","text":"<p>The script dynamically selects the appropriate SAE class:</p> <pre><code>sae_classes = {\n    \"vanilla\": VanillaSAE,\n    \"topk\": TopKSAE,\n    \"batchtopk\": BatchTopKSAE,\n    \"jumprelu\": JumpReLUSAE,\n}\nsae = sae_classes[cfg[\"sae_type\"]](cfg)\n</code></pre> <p>Each SAE receives the same configuration dictionary.</p>"},{"location":"training/#data-loading-activationsstoresbert","title":"\ud83d\udcca Data Loading: <code>ActivationsStoreSBERT</code>","text":"<p>The class <code>ActivationsStoreSBERT</code> is responsible for efficiently streaming and  buffering text data as <code>Sentence-BERT</code> embeddings.</p> <p>It performs the following tasks:</p> <ul> <li>Streams raw text from a Hugging Face dataset using <code>streaming=True</code>.</li> <li>Encodes text batches using a pre-trained Sentence-BERT model.</li> <li>Buffers multiple batches into memory to simulate a fixed-size dataset.</li> <li>Provides minibatches of embeddings for training or analysis.</li> </ul> <pre><code>activation_store = ActivationsStoreSBERT(model, cfg)\n</code></pre> <p>This design enables large-scale training without loading the entire dataset into memory. It also handles iteration limits and buffer regeneration automatically.</p>"}]}